# -*- coding: utf-8 -*-
"""quora-nlp-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11VqObCc5uyrQTwXbm-Vu-jWKY_F7pG7g

# Neural Networks and Embeddings for Natural Language Processing

Outline:

- Download the Data
- Prepare Data for Training
- Logistic Regression Model
- Feed Forward Nueral Network

Dataset: https://www.kaggle.com/c/quora-insincere-questions-classification

### Download the Data
"""

import os

os.environ['KAGGLE_CONFIG_DIR'] = '.'

!kaggle competitions download -c quora-insincere-questions-classification -f train.csv

!kaggle competitions download -c quora-insincere-questions-classification -f test.csv
!kaggle competitions download -c quora-insincere-questions-classification -f sample_submission.csv

train_fname = './train.csv.zip'
test_fname = './test.csv.zip'
sample_fname = './sample_submission.csv.zip'

import pandas as pd

train_df = pd.read_csv(train_fname)
test_df = pd.read_csv(test_fname)

train_df

test_df

sample_df = train_df.sample(100_000, random_state=42)

"""### Prepare Data for Training
Outline:

-  Convert text to TF-IDF Vectors
-  Split training & validation set
-  Convert to PyTorch tensors
"""

from sklearn.feature_extraction.text import TfidfVectorizer

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.stem.snowball import SnowballStemmer



english_stopwords = stopwords.words('english')
stemmer = SnowballStemmer(language='english')

def tokenize(text):
    return [stemmer.stem(token) for token in word_tokenize(text) if token.isalpha()]

vectorizer = TfidfVectorizer(lowercase=True, tokenizer=tokenize,
                             stop_words=english_stopwords,
                             ngram_range=(1,2), max_features=1000)

vectorizer.fit(sample_df.question_text)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# inputs = vectorizer.transform(sample_df.question_text)

targets = sample_df.target.values

test_inputs = vectorizer.transform(test_df.question_text)

"""Split training and validation set"""

from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs, targets, test_size=0.3, random_state=42)

train_inputs.shape

train_targets.shape

"""### Convert to PyTorch tensors"""

import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn.functional as F

train_tensors = F.normalize(torch.tensor(train_inputs.toarray()).float(), dim=0)
val_tensors = F.normalize(torch.tensor(val_inputs.toarray()).float(), dim=0)

train_tensors.shape, val_tensors.shape

train_ds = TensorDataset(train_tensors, torch.tensor(train_targets))
val_ds = TensorDataset(val_tensors, torch.tensor(val_targets))

batch_size = 32
train_dl = DataLoader(train_ds, batch_size, shuffle=True)
val_dl = DataLoader(val_ds, batch_size)

for inputs_batch, targets_batch in train_dl:
    print('inputs.shape', inputs_batch.shape)
    print('targets.shape', targets_batch.shape)
    print(targets_batch)
    break

"""### Logistic Regression Model"""

import torch.nn as nn

class LogReg(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(1000, 1)
    def forward(self, xb):
        out = self.linear1(xb)
        return out

import numpy as np
from sklearn.metrics import accuracy_score, f1_score

logreg_model = LogReg()

for batch in val_dl:
    batch_inputs, batch_targets = batch
    print('inputs.shape', batch_inputs.shape)
    print('targets', batch_targets)

    batch_out = logreg_model(batch_inputs)
    probs = torch.sigmoid(batch_out[:,0])
    preds = (probs >= 0.5).int()

    print('outputs', preds)
    print('accuracy', accuracy_score(batch_targets, preds))
    print('f1_score', f1_score(batch_targets, preds))
    break

def evaluate(model, dl):
    losses, accs, f1s = [], [], []
    for batch in dl:
        inputs, targets = batch
        out = model(inputs)

        probs = torch.sigmoid(out[:,0])
        loss = F.binary_cross_entropy(probs, targets.float(),
                                      weight = torch.tensor(20.))
        losses.append(loss.item())

        preds = (probs >= 0.5).int()
        acc = accuracy_score(targets, preds)
        f1 = f1_score(targets, preds)

        accs.append(acc)
        f1s.append(f1)

    return np.mean(losses), np.mean(accs), np.mean(f1s)

def fit(epochs, lr, model, train_loader, val_loader):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    history = [] # for recording epoch-wise results

    for epoch in range(epochs):

        # Training Phase
        for batch in train_loader:
            inputs, targets = batch
            outputs = model(inputs)
            probs = torch.sigmoid(outputs[:,0])
            loss = F.binary_cross_entropy(probs, targets.float(),
                                          weight=torch.tensor(20.))
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # Validation Phase
        result = evaluate(model, val_loader)
        loss, acc, f1 = result
        print('Epoch: {}; Loss: {:.4f}; Accuracy: {:.4f}; F1 Score: {:.4f}'.format(
            epoch, loss, acc, f1))
        history.append(result)

    return history

logreg_model = LogReg()

history = [evaluate(logreg_model, val_dl)]

history

history += fit(5, 0.01, logreg_model, train_dl, val_dl)

history += fit(5, 0.01, logreg_model, train_dl, val_dl)

history += fit(5, 0.01, logreg_model, train_dl, val_dl)

losses = [item[0] for item in history]

import matplotlib.pyplot as plt

plt.plot(losses);
plt.title('Loss')

f1s = [item[2] for item in history]

plt.plot(f1s);
plt.title('F1 Score')

"""## Feed Forward Neural Network"""

class FeedForwardModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1000, 512)
        self.layer2 = nn.Linear(512, 256)
        self.layer3 = nn.Linear(256, 128)
        self.layer4 = nn.Linear(128, 1)

    def forward(self, xb):
        out = F.relu(self.layer1(xb))
        out = F.relu(self.layer2(out))
        out = F.relu(self.layer3(out))
        out = self.layer4(out)
        return out

ff_model = FeedForwardModel()

history = [evaluate(ff_model, val_dl)]

history

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history += fit(5, 0.001, ff_model, train_dl, val_dl)

